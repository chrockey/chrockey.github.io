<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Chunghyun  Park</title>
    <meta name="author" content="Chunghyun  Park" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://chrockey.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://chrockey.github.io/"><span class="font-weight-bold">Chunghyun</span>   Park</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              <!--  -->
              <!-- Blog -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li> -->

              <!-- CV -->
              <li class="nav-item">
                <a class="nav-link" href="/assets/pdf/cv_chunghyun_park.pdf" target="_blank" rel="noopener noreferrer">Curriculum Vitae</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/archives/">Archives</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Chunghyun</span>  Park
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/chunghyun-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/chunghyun-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/chunghyun-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="/assets/img/chunghyun.png" alt="chunghyun.png">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I am a Ph.D. student at <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">POSTECH</a> where I am advised by Prof. <a href="http://cvlab.postech.ac.kr/~mcho/" target="_blank" rel="noopener noreferrer">Minsu Cho</a> and a member of the <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a>. Previously, I completed my M.S. in Artificial Intelligence and B.S. in Mechanical Engineering at POSTECH.</p>

<p>My research lies in machine learning and computer vision including, but not limited to, 3D scene understanding and reconstruction. I am particularly interested in 3D perception on point clouds for applications in AR/VR and robotics.</p>

<p>If you are interested in my research projects, please feel free to contact me.</p>

            <!-- Social -->
            <div class="social">
              <div class="contact-icons">
              <a href="mailto:%70%30%31%32%35%63%68@%70%6F%73%74%65%63%68.%61%63.%6B%72" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=5ABvjQcAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/chrockey" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/chunghyun-park-7a50b0170" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            
              </div>

              <div class="contact-note">
                
              </div>
              
            </div>
          </div>

          <!-- News -->          
          <div class="news">
            <h2>News</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row" style="width:16%">Feb 23, 2026</th>
                  <td>
                    <img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> <a href="https://arxiv.org/abs/2601.09211" target="_blank" rel="noopener noreferrer">Affostruction</a> is accepted to <a href="https://cvpr.thecvf.com/" target="_blank" rel="noopener noreferrer">CVPR 2026</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width:16%">Nov 25, 2025</th>
                  <td>
                    <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> I won the POSTECHIAN Fellowship (Innovation).
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width:16%">Feb 27, 2025</th>
                  <td>
                    <img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> <a href="https://arxiv.org/abs/2502.02548" target="_blank" rel="noopener noreferrer">Mosaic3D</a> is accepted to <a href="https://cvpr.thecvf.com/" target="_blank" rel="noopener noreferrer">CVPR 2025</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width:16%">May 22, 2024</th>
                  <td>
                    <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> I am selected as an <a href="https://media.eventhosts.cc/Conferences/CVPR2024/CVPR_main_conf_2024.pdf" target="_blank" rel="noopener noreferrer">outstanding reviewer</a> for CVPR 2024.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width:16%">Feb 27, 2024</th>
                  <td>
                    <img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> A paper about 3D semantic correspondence is accepted to <a href="https://cvpr.thecvf.com/" target="_blank" rel="noopener noreferrer">CVPR 2024</a>.
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          
          <!-- Selected papers -->
          <div class="publications">
            <h2>Selected Publications</h2>
            <p>* indicates equal contribution.</p>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/park2026affostruction-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/park2026affostruction-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/park2026affostruction-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/park2026affostruction.png" data-zoomable>

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="park2026affostruction" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Affostruction: 3D Affordance Grounding with Generative Reconstruction</div>
          <!-- Author -->
          <div class="author">
                  <em>Chunghyun Park</em>, <a href="https://llishyun.github.io/" target="_blank" rel="noopener noreferrer">Seunghyeon Lee</a>, and <a href="https://cvlab.postech.ac.kr/~mcho/" target="_blank" rel="noopener noreferrer">Minsu Cho</a>
                  

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2026
          </div>
          
          <!-- Additional info -->
          

          <!-- Additional info2 -->
          
              
          <!-- Additional info3 -->
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2601.09211" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://chrockey.github.io/Affostruction/" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper addresses the problem of affordance grounding from RGBD images of an object, which aims to localize surface regions corresponding to a text query that describes an action on the object. While existing methods predict affordance regions only on visible surfaces, we propose Affostruction, a generative framework that reconstructs complete geometry from partial observations and grounds affordances on the full shape including unobserved regions. We make three core contributions: generative multi-view reconstruction via sparse voxel fusion that extrapolates unseen geometry while maintaining constant token complexity, flow-based affordance grounding that captures inherent ambiguity in affordance distributions, and affordance-driven active view selection that leverages predicted affordances for intelligent viewpoint sampling. Affostruction achieves 19.1 aIoU on affordance grounding (40.4% improvement) and 32.67 IoU for 3D reconstruction (67.7% improvement), enabling accurate affordance prediction on complete shapes.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/lee2025mosaic3d-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/lee2025mosaic3d-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/lee2025mosaic3d-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/lee2025mosaic3d.png" data-zoomable>

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="lee2025mosaic3d" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation</div>
          <!-- Author -->
          <div class="author">
<a href="https://junha-l.github.io/" target="_blank" rel="noopener noreferrer">Junha Lee*</a>, 
                  <em>Chunghyun Park*</em>, <a href="https://jaesung-choe.github.io/" target="_blank" rel="noopener noreferrer">Jaesung Choe</a>, <a href="https://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank" rel="noopener noreferrer">Yu-Chiang Frank Wang</a>, <a href="https://jankautz.com/" target="_blank" rel="noopener noreferrer">Jan Kautz</a>, <a href="https://cvlab.postech.ac.kr/~mcho/" target="_blank" rel="noopener noreferrer">Minsu Cho</a>, and <a href="https://chrischoy.github.io/" target="_blank" rel="noopener noreferrer">Chris Choy</a>
                  

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2025
          </div>
          
          <!-- Additional info -->
          

          <!-- Additional info2 -->
          
              
          <!-- Additional info3 -->
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2502.02548" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://nvlabs.github.io/Mosaic3D/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We tackle open-vocabulary 3D scene segmentation tasks by introducing a novel data generation pipeline and training framework. Our work targets three essential aspects required for an effective dataset: precise 3D region segmentation, comprehensive textual descriptions, and sufficient dataset scale. By leveraging state-of-the-art open-vocabulary image segmentation models and region-aware vision-language models (VLM), we develop an automatic pipeline capable of producing high-quality 3D mask-text pairs. Applying this pipeline to multiple 3D scene datasets, we create Mosaic3D-5.6M, a dataset of more than 30K annotated scenes with 5.6M mask-text pairs, significantly larger than existing datasets. Building on these data, we propose Mosaic3D, a 3D visiual foundation model (3D-VFM) combining a 3D encoder trained with contrastive learning and a lightweight mask decoder for open-vocabulary 3D semantic and instance segmentation. Our approach achieves state-of-the-art results on open-vocabulary 3D semantic and instance segmentation benchmarks including ScanNet200, Matterport3D, and ScanNet++, with ablation studies validating the effectiveness of our large-scale training data.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/park2024learning-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/park2024learning-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/park2024learning-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/park2024learning.png" data-zoomable>

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="park2024learning" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Learning SO(3)-Invariant Semantic Correspondence via Local Shape Transform</div>
          <!-- Author -->
          <div class="author">
                  <em>Chunghyun Park*</em>, <a href="https://wookiekim.github.io/" target="_blank" rel="noopener noreferrer">Seungwook Kim*</a>, <a href="https://jaesik.info/" target="_blank" rel="noopener noreferrer">Jaesik Park</a>, and <a href="https://cvlab.postech.ac.kr/~mcho/" target="_blank" rel="noopener noreferrer">Minsu Cho</a>
                  

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2024
          </div>
          
          <!-- Additional info -->
          <span style="color:red"><b>Presented in Workshop on Equivariant Vision: From Theory to Practice @ CVPR 2024</b></span>

          <!-- Additional info2 -->
          
              
          <!-- Additional info3 -->
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2404.11156" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/chrockey/RIST" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://chrockey.github.io/RIST/" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Establishing accurate 3D correspondences between shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised Rotation-Invariant 3D correspondence learner with Local Shape Transform, dubbed RIST, that learns to establish dense correspondences between shapes even under challenging intra-class variations and arbitrary orientations. Specifically, RIST learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shapes to be mapped to similar local shape descriptors, enabling RIST to establish dense point-wise correspondences. RIST demonstrates state-of-the-art performances on 3D part label transfer and semantic keypoint transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/kim2023stable-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/kim2023stable-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/kim2023stable-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/kim2023stable.png" data-zoomable>

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="kim2023stable" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Stable and Consistent Prediction of 3D Characteristic Orientation via Invariant Residual Learning</div>
          <!-- Author -->
          <div class="author">
<a href="https://wookiekim.github.io/" target="_blank" rel="noopener noreferrer">Seungwook Kim*</a>, 
                  <em>Chunghyun Park*</em>, <a href="https://github.com/jeongyw12382" target="_blank" rel="noopener noreferrer">Yoonwoo Jeong</a>, <a href="https://jaesik.info/" target="_blank" rel="noopener noreferrer">Jaesik Park</a>, and <a href="https://cvlab.postech.ac.kr/~mcho/" target="_blank" rel="noopener noreferrer">Minsu Cho</a>
                  

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>International Conference on Machine Learning (<b>ICML</b>),</em> 2023
          </div>
          
          <!-- Additional info -->
          

          <!-- Additional info2 -->
          
              
          <!-- Additional info3 -->
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2306.11406" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Learning to predict reliable characteristic orientations of 3D point clouds is an important yet challenging problem, as different point clouds of the same class may have largely varying appearances. In this work, we introduce a novel method to decouple the shape geometry and semantics of the input point cloud to achieve both stability and consistency. The proposed method integrates shape-geometry-based SO(3)-equivariant learning and shape-semantics-based SO(3)-invariant residual learning, where a final characteristic orientation is obtained by calibrating an SO(3)-equivariant orientation hypothesis using an SO(3)-invariant residual rotation. In experiments, the proposed method not only demonstrates superior stability and consistency but also exhibits state-of-the-art performances when applied to point cloud part segmentation, given randomly rotated inputs.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/choe2022pointmixer-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/choe2022pointmixer-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/choe2022pointmixer-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/choe2022pointmixer.jpeg" data-zoomable>

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="choe2022pointmixer" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">PointMixer: MLP-Mixer for Point Cloud Understanding</div>
          <!-- Author -->
          <div class="author">
<a href="https://jaesung-choe.github.io/" target="_blank" rel="noopener noreferrer">Jaesung Choe*</a>, 
                  <em>Chunghyun Park*</em>, <a href="https://rameau-fr.github.io/" target="_blank" rel="noopener noreferrer">Francois Rameau</a>, <a href="https://jaesik.info/" target="_blank" rel="noopener noreferrer">Jaesik Park</a>, and <a href="https://scholar.google.com/citations?user=XA8EOlEAAAAJ&amp;hl=ko" target="_blank" rel="noopener noreferrer">In So Kweon</a>
                  

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>European Conference on Computer Vision (<b>ECCV</b>),</em> 2022
          </div>
          
          <!-- Additional info -->
          

          <!-- Additional info2 -->
          
              
          <!-- Additional info3 -->
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2111.11187" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/LifeBeyondExpectations/ECCV22-PointMixer" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>MLP-Mixer has newly appeared as a new challenger against the realm of CNNs and Transformer. Despite its simplicity compared to Transformer, the concept of channel-mixing MLPs and token-mixing MLPs achieves noticeable performance in image recognition tasks. Unlike images, point clouds are inherently sparse, unordered and irregular, which limits the direct use of MLP-Mixer for point cloud understanding. To overcome these limitations, we propose PointMixer, a universal point set operator that facilitates information sharing among unstructured 3D point cloud. By simply replacing token-mixing MLPs with Softmax function, PointMixer can “mix" features within/between point sets. By doing so, PointMixer can be broadly used for intra-set, inter-set, and hierarchical-set mixing. We demonstrate that various channel-wise feature aggregation in numerous point sets is better than self-attention layers or dense token-wise interaction in a view of parameter efficiency and accuracy. Extensive experiments show the competitive or superior performance of PointMixer in semantic segmentation, classification, and reconstruction against Transformer-based methods.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/park2022fast-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/park2022fast-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/park2022fast-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/park2022fast.png" data-zoomable>

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="park2022fast" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Fast Point Transformer</div>
          <!-- Author -->
          <div class="author">
                  <em>Chunghyun Park</em>, <a href="https://github.com/jeongyw12382" target="_blank" rel="noopener noreferrer">Yoonwoo Jeong</a>, <a href="https://cvlab.postech.ac.kr/~mcho/" target="_blank" rel="noopener noreferrer">Minsu Cho</a>, and <a href="https://jaesik.info/" target="_blank" rel="noopener noreferrer">Jaesik Park</a>
                  

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2022
          </div>
          
          <!-- Additional info -->
          <span style="color:red"><b>Silver Prize @ Samsung HumanTech Paper Award</b></span>

          <!-- Additional info2 -->
          <br><span style="color:red"><b>Qualcomm Innovation Fellowship Winner</b></span>
              
          <!-- Additional info3 -->
          
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2112.04702" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/POSTECH-CVLab/FastPointTransformer" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cvlab.postech.ac.kr/research/FPT/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The recent success of neural networks enables a better interpretation of 3D point clouds, but processing a large-scale 3D scene remains a challenging problem. Most current approaches divide a large-scale scene into small regions and combine the local predictions together. However, this scheme inevitably involves additional stages for pre- and post-processing and may also degrade the final output due to predictions in a local perspective. This paper introduces Fast Point Transformer that consists of a new lightweight self-attention layer. Our approach encodes continuous 3D coordinates, and the voxel hashing-based architecture boosts computational efficiency. The proposed method is demonstrated with 3D semantic segmentation and 3D detection. The accuracy of our approach is competitive to the best voxel-based method, and our network achieves 129 times faster inference time than the state-of-the-art, Point Transformer, with a reasonable accuracy trade-off in 3D semantic segmentation on S3DIS dataset.</p>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>

          
          
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2026 Chunghyun  Park. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

